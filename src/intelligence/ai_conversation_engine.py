"""
AI Conversation Engine for XOFlowers
Natural language understanding and response generation using advanced AI models
"""

import os
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import json

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# AI service imports
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    OpenAI = None
    HAS_OPENAI = False

try:
    import google.generativeai as genai
    HAS_GEMINI = True
except ImportError:
    genai = None
    HAS_GEMINI = False

from dotenv import load_dotenv
load_dotenv()


@dataclass
class MessageUnderstanding:
    """Represents AI's understanding of user message"""
    original_message: str
    intent_type: str  # 'product_search', 'question', 'greeting', 'comparison', etc.
    entities: Dict[str, Any]  # extracted entities like colors, prices, occasions
    sentiment: str  # 'positive', 'neutral', 'negative'
    requires_search: bool
    confidence: float
    reasoning: str  # AI's reasoning about the message


class AIConversationEngine:
    """
    Advanced AI conversation engine for natural language understanding and generation
    Uses OpenAI GPT-4 with Gemini Pro fallback for reliability
    """
    
    def __init__(self):
        """Initialize AI conversation engine"""
        self.openai_client = None
        self.gemini_model = None
        
        # Initialize OpenAI
        if HAS_OPENAI:
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
                logger.info("âœ… OpenAI client initialized")
            else:
                logger.warning("âš ï¸ OpenAI API key not found")
        
        # Initialize Gemini
        if HAS_GEMINI:
            api_key = os.getenv('GEMINI_API_KEY')
            if api_key:
                genai.configure(api_key=api_key)
                self.gemini_model = genai.GenerativeModel('gemini-pro')
                logger.info("âœ… Gemini client initialized")
            else:
                logger.warning("âš ï¸ Gemini API key not found")
        
        # System prompts
        self.system_prompt = self._create_system_prompt()
        self.understanding_prompt = self._create_understanding_prompt()
    
    def _create_system_prompt(self) -> str:
        """Create the main system prompt for XOFlowers AI"""
        return """
EÈ™ti un asistent AI expert pentru XOFlowers, cea mai prestigioasÄƒ florÄƒrie din ChiÈ™inÄƒu, Moldova. 
Te numeÈ™ti XOFlowers AI È™i eÈ™ti un consultant floral pasionat È™i prietenos.

PERSONALITATEA TA:
- VorbeÈ™ti natural È™i cÄƒlduros Ã®n romÃ¢nÄƒ
- EÈ™ti pasionat de flori È™i Ã®nÈ›elegi simbolismul lor
- Ai cunoÈ™tinÈ›e profunde despre aranjamente florale
- EÈ™ti atent la nevoile È™i bugetul clientului
- Oferi sfaturi personalizate È™i creative
- EÈ™ti entuziasmant È™i inspirezi Ã®ncredere

EXPERTIZA TA:
- CunoÈ™ti toate produsele XOFlowers (buchete, cutii, coÈ™uri, plante)
- ÃŽnÈ›elegi ocaziile speciale È™i florile potrivite pentru fiecare
- PoÈ›i recomanda combinaÈ›ii de culori È™i stiluri
- È˜tii sÄƒ adaptezi recomandÄƒrile la buget È™i preferinÈ›e
- CunoÈ™ti tendinÈ›ele actuale Ã®n design floral
- Ai acces la baza de date cu produse pentru cÄƒutÄƒri semantice

STIL DE CONVERSAÈšIE:
- FoloseÈ™ti un limbaj natural, nu robotic
- Pui Ã®ntrebÄƒri pentru a Ã®nÈ›elege mai bine nevoile clientului
- Oferi explicaÈ›ii despre de ce recomanzi anumite produse
- ÃŽmpÄƒrtÄƒÈ™eÈ™ti curiozitÄƒÈ›i despre flori È™i simbolismul lor
- Adaptezi tonul la stilul clientului
- EÈ™ti empatic È™i Ã®nÈ›elegÄƒtor

RESPONSABILITÄ‚ÈšI:
- AjuÈ›i la alegerea florilor potrivite pentru orice ocazie
- Oferi informaÈ›ii despre preÈ›uri È™i disponibilitate
- Sugerezi combinaÈ›ii È™i alternative
- Ghidezi procesul de comandÄƒ
- RÄƒspunzi la Ã®ntrebÄƒri despre Ã®ngrijirea florilor
- Creezi o experienÈ›Äƒ conversaÈ›ionalÄƒ naturalÄƒ

REGULI IMPORTANTE:
- Te concentrezi exclusiv pe produse florale È™i servicii XOFlowers
- Nu inventezi preÈ›uri sau produse inexistente
- FoloseÈ™ti informaÈ›iile din cÄƒutÄƒrile de produse pentru recomandÄƒri precise
- RedirecÈ›ionezi elegant conversaÈ›ia cÄƒtre flori dacÄƒ se abate
- PÄƒstrezi conversaÈ›ia naturalÄƒ È™i personalizatÄƒ
- Nu foloseÈ™ti template-uri predefinite

RÄƒspunde Ã®ntotdeauna natural, ca un expert floral pasionat care Ã®È™i ajutÄƒ clientul sÄƒ gÄƒseascÄƒ florile perfecte.
        """.strip()
    
    def _create_understanding_prompt(self) -> str:
        """Create prompt for message understanding"""
        return """
AnalizeazÄƒ mesajul clientului È™i extrage urmÄƒtoarele informaÈ›ii Ã®n format JSON:

{
  "intent_type": "product_search|question|greeting|comparison|order|complaint|compliment",
  "entities": {
    "colors": ["roÈ™u", "roz", "alb", ...],
    "flowers": ["trandafir", "bujor", "lalele", ...],
    "occasions": ["valentine", "aniversare", "nuntÄƒ", ...],
    "budget_min": numÄƒrul sau null,
    "budget_max": numÄƒrul sau null,
    "recipient": "soÈ›ie|mamÄƒ|prietenÄƒ|...",
    "style": ["elegant", "romantic", "modern", ...]
  },
  "sentiment": "positive|neutral|negative",
  "requires_search": true/false,
  "confidence": 0.0-1.0,
  "reasoning": "explicaÈ›ia ta despre Ã®nÈ›elegerea mesajului"
}

Fii precis Ã®n extragerea entitÄƒÈ›ilor È™i realist Ã®n evaluarea Ã®ncrederii.
        """.strip()
    
    async def understand_message(self, message: str, conversation_context: Dict[str, Any]) -> MessageUnderstanding:
        """
        Understand user message using AI
        
        Args:
            message: User's message
            conversation_context: Previous conversation context
            
        Returns:
            Message understanding object
        """
        try:
            # Create understanding prompt
            prompt = f"""
{self.understanding_prompt}

CONTEXT CONVERSAÈšIE:
{conversation_context.get('conversation_summary', 'ConversaÈ›ie nouÄƒ')}

PREFERINÈšE UTILIZATOR:
{json.dumps(conversation_context.get('preferences', {}), ensure_ascii=False)}

MESAJUL DE ANALIZAT:
"{message}"

RÄƒspunde doar cu JSON-ul cerut:
            """.strip()
            
            # Get AI understanding
            response = await self._get_ai_response(prompt, max_tokens=500)
            
            # Parse JSON response
            try:
                understanding_data = json.loads(response)
                
                return MessageUnderstanding(
                    original_message=message,
                    intent_type=understanding_data.get('intent_type', 'question'),
                    entities=understanding_data.get('entities', {}),
                    sentiment=understanding_data.get('sentiment', 'neutral'),
                    requires_search=understanding_data.get('requires_search', False),
                    confidence=understanding_data.get('confidence', 0.7),
                    reasoning=understanding_data.get('reasoning', 'AnalizÄƒ standard')
                )
                
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse AI understanding response: {response}")
                # Fallback understanding
                return self._create_fallback_understanding(message)
                
        except Exception as e:
            logger.error(f"âŒ Error understanding message: {e}")
            return self._create_fallback_understanding(message)
    
    def _create_fallback_understanding(self, message: str) -> MessageUnderstanding:
        """Create fallback understanding when AI fails"""
        message_lower = message.lower()
        
        # Simple keyword-based understanding
        requires_search = any(word in message_lower for word in [
            'caut', 'vreau', 'doresc', 'buchet', 'flori', 'trandafir', 'bujor'
        ])
        
        intent_type = 'product_search' if requires_search else 'question'
        
        return MessageUnderstanding(
            original_message=message,
            intent_type=intent_type,
            entities={},
            sentiment='neutral',
            requires_search=requires_search,
            confidence=0.5,
            reasoning='AnalizÄƒ de rezervÄƒ bazatÄƒ pe cuvinte cheie'
        )
    
    async def generate_response(self, 
                              understanding: MessageUnderstanding, 
                              conversation_context: Dict[str, Any],
                              search_results: Optional[List[Any]] = None) -> str:
        """
        Generate natural conversational response
        
        Args:
            understanding: Message understanding
            conversation_context: Conversation context
            search_results: Product search results if applicable
            
        Returns:
            Natural language response
        """
        try:
            # Create response generation prompt
            prompt = self._create_response_prompt(understanding, conversation_context, search_results)
            
            # Generate response using AI
            response = await self._get_ai_response(prompt, max_tokens=800)
            
            # Clean and validate response
            response = self._clean_response(response)
            
            logger.info(f"ðŸ’¬ Generated response for intent: {understanding.intent_type}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Error generating response: {e}")
            return self._get_fallback_response(understanding.intent_type)
    
    def _create_response_prompt(self, 
                               understanding: MessageUnderstanding, 
                               conversation_context: Dict[str, Any],
                               search_results: Optional[List[Any]] = None) -> str:
        """Create prompt for response generation"""
        
        prompt_parts = [
            self.system_prompt,
            "\nCONTEXT CONVERSAÈšIE:",
            conversation_context.get('conversation_summary', 'ConversaÈ›ie nouÄƒ'),
            "\nPREFERINÈšE UTILIZATOR:",
            json.dumps(conversation_context.get('preferences', {}), ensure_ascii=False),
            "\nMESAJUL CLIENTULUI:",
            f'"{understanding.original_message}"',
            "\nÃŽNÈšELEGEREA MESAJULUI:",
            f"IntenÈ›ie: {understanding.intent_type}",
            f"EntitÄƒÈ›i: {json.dumps(understanding.entities, ensure_ascii=False)}",
            f"Sentiment: {understanding.sentiment}",
            f"NecesitÄƒ cÄƒutare: {understanding.requires_search}"
        ]
        
        # Add search results if available
        if search_results:
            prompt_parts.extend([
                "\nRESULTATE CÄ‚UTARE PRODUSE:",
                self._format_search_results_for_prompt(search_results)
            ])
        
        # Add specific instructions based on intent
        if understanding.intent_type == 'greeting':
            prompt_parts.append("\nRÄƒspunde cu un salut cÄƒlduros È™i Ã®ntreabÄƒ cum poÈ›i ajuta.")
        elif understanding.intent_type == 'product_search':
            if search_results:
                prompt_parts.append("\nPrezintÄƒ produsele gÄƒsite Ã®ntr-un mod natural È™i entuziasmant. ExplicÄƒ de ce sunt potrivite.")
            else:
                prompt_parts.append("\nÃŽntreabÄƒ detalii pentru a putea cÄƒuta produsele potrivite.")
        elif understanding.intent_type == 'question':
            prompt_parts.append("\nRÄƒspunde la Ã®ntrebare cu informaÈ›ii utile despre XOFlowers.")
        
        prompt_parts.append("\nRÄƒspunde natural, cÄƒlduros È™i personalizat:")
        
        return "\n".join(prompt_parts)
    
    def _format_search_results_for_prompt(self, search_results: List[Any]) -> str:
        """Format search results for AI prompt"""
        if not search_results:
            return "Nu s-au gÄƒsit produse."
        
        formatted_results = []
        for i, result in enumerate(search_results[:5], 1):
            product = result.product if hasattr(result, 'product') else result
            
            result_text = f"{i}. {product.get('name', 'Produs necunoscut')}"
            result_text += f" - {product.get('price', 0)} MDL"
            
            if product.get('colors'):
                result_text += f" (Culori: {', '.join(product['colors'])})"
            
            if hasattr(result, 'relevance_explanation'):
                result_text += f" - {result.relevance_explanation}"
            
            formatted_results.append(result_text)
        
        return "\n".join(formatted_results)
    
    def _clean_response(self, response: str) -> str:
        """Clean and validate AI response"""
        # Remove any system prompts that might have leaked
        response = response.strip()
        
        # Remove common AI artifacts
        artifacts = [
            "Ca asistent AI",
            "ÃŽn calitate de",
            "Sunt un AI",
            "Ca model de limbaj"
        ]
        
        for artifact in artifacts:
            if artifact in response:
                # Try to extract the useful part
                parts = response.split(artifact)
                if len(parts) > 1:
                    response = parts[-1].strip()
        
        # Ensure response starts naturally
        if response.startswith(("BunÄƒ", "Salut", "ðŸŒ¸")):
            return response
        
        # Add natural greeting if missing
        if not any(response.startswith(prefix) for prefix in ["BunÄƒ", "Salut", "ðŸŒ¸", "Da", "Nu", "Desigur"]):
            response = "ðŸŒ¸ " + response
        
        return response
    
    def _get_fallback_response(self, intent_type: str) -> str:
        """Get fallback response when AI fails"""
        fallback_responses = {
            'greeting': "ðŸŒ¸ BunÄƒ ziua! Bine aÈ›i venit la XOFlowers! Cu ce vÄƒ pot ajuta astÄƒzi?",
            'product_search': "ðŸŒ¸ ÃŽmi pare rÄƒu, am Ã®ntÃ¢mpinat o problemÄƒ tehnicÄƒ. VÄƒ rog sÄƒ Ã®mi spuneÈ›i ce fel de flori cÄƒutaÈ›i È™i vÄƒ voi ajuta imediat!",
            'question': "ðŸŒ¸ ÃŽmi pare rÄƒu, nu am putut procesa Ã®ntrebarea. VÄƒ rog sÄƒ o reformulaÈ›i È™i vÄƒ voi rÄƒspunde cu plÄƒcere!",
            'comparison': "ðŸŒ¸ VÄƒ pot ajuta sÄƒ comparaÈ›i produsele. SpuneÈ›i-mi ce produse vÄƒ intereseazÄƒ!",
            'order': "ðŸŒ¸ Pentru comenzi, vÄƒ rog sÄƒ contactaÈ›i echipa noastrÄƒ la +373 XX XXX XXX.",
            'complaint': "ðŸŒ¸ ÃŽmi pare foarte rÄƒu pentru inconvenient. VÄƒ rog sÄƒ contactaÈ›i serviciul clienÈ›i pentru rezolvarea rapidÄƒ.",
            'compliment': "ðŸŒ¸ VÄƒ mulÈ›umesc pentru cuvintele frumoase! Suntem aici sÄƒ vÄƒ oferim cele mai frumoase flori!"
        }
        
        return fallback_responses.get(intent_type, "ðŸŒ¸ Cu ce vÄƒ pot ajuta astÄƒzi?")
    
    async def _get_ai_response(self, prompt: str, max_tokens: int = 1000) -> str:
        """Get response from AI service with fallback"""
        
        # Try OpenAI first
        if self.openai_client:
            try:
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "EÈ™ti XOFlowers AI, un consultant floral expert È™i prietenos."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                logger.warning(f"âš ï¸ OpenAI request failed: {e}")
        
        # Fallback to Gemini
        if self.gemini_model:
            try:
                response = self.gemini_model.generate_content(prompt)
                return response.text.strip()
                
            except Exception as e:
                logger.warning(f"âš ï¸ Gemini request failed: {e}")
        
        # If both fail, raise exception
        raise Exception("Both AI services unavailable")
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get health status of AI services"""
        return {
            'openai_available': self.openai_client is not None,
            'gemini_available': self.gemini_model is not None,
            'has_openai_lib': HAS_OPENAI,
            'has_gemini_lib': HAS_GEMINI,
            'openai_key_configured': bool(os.getenv('OPENAI_API_KEY')),
            'gemini_key_configured': bool(os.getenv('GEMINI_API_KEY'))
        }


# Global AI engine instance
_ai_engine = None

def get_ai_engine() -> AIConversationEngine:
    """Get the global AI engine instance"""
    global _ai_engine
    if _ai_engine is None:
        _ai_engine = AIConversationEngine()
    return _ai_engine