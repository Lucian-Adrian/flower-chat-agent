# Design Document

## Overview

The simplified XOFlowers AI Agent will be rebuilt as a pure AI-driven conversational system, eliminating all keyword matching and template responses. The system follows the example_flow pattern with a clean Flask API that processes messages through AI-powered intent understanding, context management, and natural response generation.

## Architecture

### Core Principles
- **AI-First**: All understanding and responses generated by AI models
- **No Templates**: Zero hardcoded response templates or keyword matching
- **Modular Design**: Reusable functions with centralized definitions
- **Simple Flow**: Message → AI Security Check → AI Intent → AI Response → Delivery

### System Flow
```
User Message → FastAPI API → AI Security Check → AI Intent Analysis → Context Retrieval → AI Response Generation → Context Update → Response Delivery
```

## Components and Interfaces

### 1. FastAPI Layer (`src/api/`)
**Purpose**: Handle incoming messages from Telegram/Instagram and coordinate the AI processing pipeline using modern FastAPI framework for performance and async support.

**Key Files**:
- `main.py` - Main FastAPI application with async support
- `telegram_integration.py` - Telegram webhook handler
- `instagram_integration.py` - Instagram webhook handler

**Interface**:
```python
@app.post("/api/chat")
async def process_message(request: MessageRequest):
    # Extract message and user_id
    # Call AI pipeline asynchronously
    # Return response
```

**Benefits of FastAPI**:
- Native async/await support for better performance
- Automatic API documentation generation
- Pydantic data validation
- Better error handling and type safety

### 2. AI Processing Engine (`src/intelligence/`)
**Purpose**: Core AI-powered message processing with security, intent analysis, and response generation.

**Key Files**:
- `ai_engine.py` - Main AI processing coordinator
- `security_ai.py` - AI-powered security and jailbreak detection
- `context_manager.py` - Redis-based conversation context
- `product_search.py` - ChromaDB integration for product recommendations

**Main Interface**:
```python
def process_message_ai(user_message: str, user_id: str) -> dict:
    """
    Main AI processing pipeline
    Returns: {
        "response": str,
        "success": bool,
        "context_updated": bool
    }
    """
```

### 3. Data Access Layer (`src/data/`)
**Purpose**: Access to business information, FAQ data, and product database.

**Key Files**:
- `faq_manager.py` - Access to FAQ and business info from JSON
- `chromadb_client.py` - Product search in ChromaDB
- `redis_client.py` - Conversation context storage

### 4. System Definitions (`src/helpers/`)
**Purpose**: Centralized configuration and reusable utilities - single source of truth for all system constants.

**Key Files**:
- `system_definitions.py` - Single source of truth for all constants, AI prompts, service configurations, and system settings
- `utils.py` - Shared utility functions including logging configuration

## Data Models

### Message Processing Model
```python
@dataclass
class MessageContext:
    user_id: str
    message_text: str
    platform: str  # "telegram" or "instagram"
    timestamp: datetime
    conversation_history: List[dict]
```

### AI Response Model
```python
@dataclass
class AIResponse:
    response_text: str
    intent_detected: str
    confidence: float
    products_included: List[dict]
    context_updated: bool
    processing_time: float
```

### Security Check Model
```python
@dataclass
class SecurityResult:
    is_safe: bool
    risk_level: str  # "low", "medium", "high"
    detected_issues: List[str]
    should_proceed: bool
```

## Error Handling

### AI Service Fallback Chain
1. **Primary**: OpenAI GPT-4o-mini (fast and cost-effective)
2. **Fallback**: Google Gemini Pro (if OpenAI fails)
3. **Emergency**: Predefined safe response (if both fail)

### Database Fallback Strategy
1. **ChromaDB Unavailable**: Use FAQ data only, inform user about limited search
2. **Redis Unavailable**: Process without context, log warning
3. **FAQ Data Missing**: Use system_definitions backup info

### Response Generation Safeguards
```python
def generate_safe_response(message: str, context: dict) -> str:
    """
    Multi-layer response generation with fallbacks:
    1. Try OpenAI with full context
    2. Try Gemini with simplified context  
    3. Return safe default response
    """
```

## Testing Strategy

### Unit Testing
- **AI Engine Tests**: Mock AI responses, test processing logic
- **Security Tests**: Test jailbreak detection with known patterns
- **Context Tests**: Redis operations and conversation management
- **Data Access Tests**: FAQ retrieval and ChromaDB queries

### Integration Testing
- **End-to-End Flow**: Message → AI Processing → Response
- **Platform Integration**: Telegram and Instagram webhook handling
- **Fallback Testing**: AI service failures and recovery
- **Performance Testing**: Response time under load

### AI Response Quality Testing
- **Intent Accuracy**: Test AI understanding of various message types
- **Response Relevance**: Ensure responses match user intent
- **Context Continuity**: Multi-turn conversation testing
- **Security Effectiveness**: Jailbreak attempt resistance

## Implementation Details

### AI Prompt Strategy
Following the example_flow pattern, we'll use carefully crafted system prompts:

```python
SECURITY_PROMPT = """
You are a security filter for XOFlowers flower shop. 
Analyze if this message is appropriate for a flower business conversation.
Return JSON: {"is_safe": true/false, "reason": "explanation"}
"""

MAIN_SYSTEM_PROMPT = """
You are the AI assistant for XOFlowers, the premium flower shop in Chișinău, Moldova.
You speak naturally in Romanian and help customers with:
- Flower recommendations and product information
- Business information (hours, location, contact)
- Order assistance and general questions

You have access to:
- Product database for recommendations
- Business FAQ information
- Conversation context

Always respond naturally and helpfully, staying focused on flowers and XOFlowers business.
"""
```

### Redis Context Structure
```python
# Key: f"xoflowers:context:{user_id}"
# Value: JSON with conversation history
{
    "user_id": "123456",
    "messages": [
        {
            "user": "Vreau trandafiri roșii",
            "assistant": "Am găsit câteva opțiuni frumoase...",
            "timestamp": "2025-07-16T10:30:00",
            "intent": "product_search"
        }
    ],
    "preferences": {
        "budget_range": [200, 800],
        "preferred_colors": ["roșu", "roz"],
        "occasions": ["romantic"]
    },
    "last_updated": "2025-07-16T10:30:00"
}
```

### ChromaDB Integration
Maintain existing ChromaDB structure but simplify access:
```python
def search_products_ai(query: str, context: dict) -> List[dict]:
    """
    AI-enhanced product search:
    1. Use AI to extract search parameters from natural language
    2. Search ChromaDB with semantic similarity
    3. Return structured product data for AI response generation
    """
```

### Service Configuration (Centralized in system_definitions.py)
```python
# Service Configurations - Single source for all connections
SERVICE_CONFIG = {
    'redis': {
        'host': 'localhost',
        'port': 6379,
        'db': 0,
        'decode_responses': True
    },
    'chromadb': {
        'path': './chroma_db_flowers',
        'collection_name': 'xoflowers_products'
    },
    'openai': {
        'model': 'gpt-4o-mini',
        'temperature': 0.1,
        'max_tokens': 1000
    },
    'gemini': {
        'model': 'gemini-pro',
        'temperature': 0.1
    }
}
```

### Logging Strategy
Comprehensive logging at every operation using Python's standard logging module:

```python
# In src/helpers/utils.py
def setup_logger(name: str) -> logging.Logger:
    """
    Configure consistent logging across all modules
    Logs to both console and file with structured format
    """
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # File handler
    file_handler = logging.FileHandler('logs/xoflowers_ai.log')
    file_handler.setLevel(logging.DEBUG)
    
    # Formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger
```

**Logging Points**:
- Message reception and user identification
- AI security check results and reasoning
- Intent analysis results and confidence scores
- Context retrieval and updates
- Product search queries and results
- AI response generation timing
- Error conditions and fallback activations

### Performance Optimizations
- **Async Processing**: Use asyncio for AI API calls
- **Connection Pooling**: Reuse Redis and ChromaDB connections
- **Response Caching**: Cache common business info responses in Redis
- **Context Compression**: Limit conversation history to last 10 exchanges
- **Request Batching**: Group multiple AI calls when possible

This design eliminates the complexity of the current system while maintaining all core functionality through AI-powered processing, exactly as requested. The centralized configuration and comprehensive logging ensure maintainability and observability.